{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from typing import Iterable, List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "import requests\n",
    "from lxml import html, etree\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection for the Irish Schools Collection\n",
    "\n",
    "This notebook implements a complete, reproducible pipeline for constructing a structured research dataset from the *Irish Schools Collection* hosted on Dúchas (<https://www.duchas.ie>). The objective is to transform the raw collection—distributed across hundreds of school-level landing pages and thousands of semi-structured HTML/XML documents—into a clean, analyzable dataset that can support downstream work in cultural analytics, historical research, linguistic variation, geospatial modeling, and computational folklore studies.\n",
    "\n",
    "## Background: The Irish Schools Collection\n",
    "\n",
    "Between 1937 and 1939, more than 100,000 Irish schoolchildren participated in a remarkable grassroots folklore project coordinated by the Irish Folklore Commission. Pupils interviewed elders in their families and communities, recording stories, customs, superstitions, local histories, material culture, folk medicine, and everyday beliefs. The resulting manuscripts—over half a million pages—constitute one of the largest folk-ethnographic collections in Europe.\n",
    "\n",
    "The digitized archive hosted by Dúchas provides access to scanned pages, HTML transcriptions, and TEI/XML representations. Journalistic and scholarly analyses have emphasized the archive’s breadth and strangeness—including the *Irish Times* profile of “Ireland’s darkest, oddest, and weirdest secrets” (<https://www.irishtimes.com/life-and-style/people/ireland-s-darkest-oddest-and-weirdest-secrets-uncovered-1.3418059>) and the National Folklore Collection’s historical introduction to the Schools Collection (<https://www.duchas.ie/download/17.01.26-irish-folklore-and-tradition.pdf>). Researchers have also drawn on this material for domain-specific studies, such as network analyses of folk-medicinal knowledge (e.g., *Frontiers in Pharmacology*, <https://www.frontiersin.org/articles/10.3389/fphar.2020.584595/full>).\n",
    "\n",
    "For computational work, the archive is both an opportunity and a technical challenge: its metadata are spread across multiple HTML views, XML endpoints, and external services such as Logainm (the official Irish placenames database). Moreover, its structure is hierarchical:\n",
    "\n",
    "- **School → Page → Item**,  \n",
    "- with each level containing partially overlapping metadata.\n",
    "\n",
    "This notebook constructs the foundational dataset required to navigate this multi-level structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "## Overview of the pipeline\n",
    "\n",
    "The workflow implemented here has five stages, corresponding to the major sections of the notebook:\n",
    "\n",
    "1. Construct a school-level dataset by crawling the schools index pages and extracting basic metadata for each school.\n",
    "2. Enrich each school with additional metadata from the Dúchas XML endpoints and associated Logainm entries.\n",
    "3. Use this enriched school dataset as a basis for collecting item-level titles and item URLs for stories, essays, and other materials.\n",
    "4. For each item, retrieve and parse both the HTML and XML representations to extract text and structured features.\n",
    "5. Estimate request volume and runtime, and benchmark subsets of the pipeline to ensure that the full crawl is both feasible and polite to the host servers.\n",
    "\n",
    "The rest of the notebook is organized as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Schools index crawl (`df_schools`)\n",
    "\n",
    "In Section 1, we crawl all publicly available Schools Collection index pages:\n",
    "\n",
    "```\n",
    "https://www.duchas.ie/en/cbes/schools?page=<page>\n",
    "```\n",
    "\n",
    "From each page we extract:\n",
    "\n",
    "* a unique school identifier (SchoolID)\n",
    "* the school URL on Dúchas\n",
    "* the school name\n",
    "* any visible CBES volume identifier (for example, “CBES 0038C”)\n",
    "* the reported percentage of material transcribed\n",
    "* the raw card text for reference\n",
    "\n",
    "We also implement a small discovery routine to find the actual range of valid index pages (rather than hard-coding an upper bound), and we add basic deduplication. The output of this stage is a dataframe, `df_schools`, with one row per school.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: School XML and Logainm enrichment (`df_schools_enriched` / `df_schools_full`)\n",
    "\n",
    "In Section 2, we enrich each school using its XML endpoint:\n",
    "\n",
    "```\n",
    "https://www.duchas.ie/xml/cbes/<SchoolID>\n",
    "```\n",
    "\n",
    "For each school we:\n",
    "\n",
    "* fetch and parse the XML document\n",
    "* extract teacher names (Irish and English, when available)\n",
    "* extract the school roll number\n",
    "* detect any Logainm URL associated with the school\n",
    "* fetch the corresponding Logainm page and, when present, extract a WKT geometry from data-wkt attributes\n",
    "\n",
    "This stage uses a single HTTP session with retry logic, backoff for transient errors and 429 rate limits, and a small throttle for politeness. The result is an enriched school-level dataframe, df_schools_enriched, which is then merged back into the original `df_schools` to produce `df_schools_full`. This enriched table is the main backbone for any spatial analyses or school-level comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Item index extraction (`df_items_index`)\n",
    "\n",
    "In Section 3, we move from schools to items. Each school’s landing page links to the individual items (stories, essays, reports, etc.) recorded there. These links typically encode school, page, and item identifiers in the URL.\n",
    "\n",
    "Here we:\n",
    "\n",
    "* visit each school’s CBES page\n",
    "* parse all relevant item links\n",
    "* extract SchoolID, PageID, ItemID, the full item URL, and the displayed item title\n",
    "\n",
    "These rows are combined into a single dataframe, df_items_index, which indexes all items in the collection that are reachable from the schools pages. This representation (SchoolID, PageID, ItemID, ItemURL, ItemTitle) provides the basic item-level graph on top of the school-level backbone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Per-item HTML and XML scraping (`df_items_full`)\n",
    "\n",
    "In Section 4, we enrich each item in `df_items_index` using its HTML and XML representations.\n",
    "\n",
    "For each item we:\n",
    "\n",
    "* fetch the HTML item page and extract:\n",
    "\n",
    "  * the main display title on the item page (which may differ from the index title)\n",
    "  * the main transcribed text\n",
    "  * any other easily accessible visible metadata\n",
    "* construct the parallel XML URL for the item and fetch it\n",
    "* parse the XML to extract basic structural and linguistic features, such as:\n",
    "\n",
    "  * token counts (for example, counts of word elements)\n",
    "  * language metadata\n",
    "  * any other TEI fields we choose to include\n",
    "\n",
    "The result is a dataframe df_items_full that combines the index-level identifiers with HTML- and XML-derived fields. This is the table you would use for most downstream text analysis, narrative modeling, or linguistic work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Request budgeting and benchmarking\n",
    "\n",
    "In Section 5, we treat the pipeline as an engineering project and ask: how expensive is a full run?\n",
    "\n",
    "We:\n",
    "\n",
    "* estimate the number of HTTP requests implied by a full item-level scrape (HTML plus XML per item)\n",
    "* benchmark the per-item enrichment on a small random subset of items to measure time per item\n",
    "* project total runtime for the full df_items_index\n",
    "* use these estimates to choose safe throttling parameters and, if necessary, to split the pipeline into batches\n",
    "\n",
    "This makes the computational cost and network load explicit and helps ensure the pipeline is both reproducible and responsible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of core dataframes\n",
    "\n",
    "By the end of this notebook, the main “products” of the pipeline are:\n",
    "\n",
    "* `df_schools`: basic school-level metadata from the index crawl\n",
    "* `df_schools_enriched` and `df_schools_full`: school-level metadata enriched with XML fields and Logainm-derived geography\n",
    "* `df_items_index`: an index of all items per school, with item URLs and titles\n",
    "* `df_items_full`: item-level records with text and structured metadata from both HTML and XML\n",
    "\n",
    "Together, these tables form a coherent, relational dataset for the Irish Schools Collection that is suitable for geospatial analyses, network analyses of narratives or informants, language variation studies, and broader computational and digital humanities research on Irish folklore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "## 1. Schools index crawl\n",
    "\n",
    "The goal of this section is to construct a school-level dataframe `df_schools` by crawling the public index pages under\n",
    "\n",
    "`https://www.duchas.ie/en/cbes/schools?page=<page>`\n",
    "\n",
    "Each index page contains a set of \"cards\" or list items, one per school. From each card we extract:\n",
    "\n",
    "- a unique school identifier,\n",
    "- the school URL on Dúchas,\n",
    "- the school name,\n",
    "- the CBES volume identifier, and\n",
    "- the reported percentage of the material that has been transcribed.\n",
    "\n",
    "This section is deliberately self-contained: it defines helper functions to build index URLs, parse a single index page into a list of rows, and then loop over all pages to build a single dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "SCHOOLS_SESSION = requests.Session()\n",
    "SCHOOLS_SESSION.headers.update(HEADERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.duchas.ie\"\n",
    "SCHOOLS_INDEX_TEMPLATE = BASE_URL + \"/en/cbes/schools?Page={page}&PerPage=20\"\n",
    "SCHOOL_PAGE_RANGE = range(1, 226)  # example: pages 1–325\n",
    "\n",
    "\n",
    "def build_school_index_url(page):\n",
    "    \"\"\"\n",
    "    Build the URL for a single schools index page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    page : int\n",
    "        Page number as used in the Dúchas schools index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Fully-qualified URL for the given index page.\n",
    "    \"\"\"\n",
    "    return SCHOOLS_INDEX_TEMPLATE.format(page=page)\n",
    "\n",
    "\n",
    "def _clean_spaces(text):\n",
    "    \"\"\"\n",
    "    Normalize whitespace within a text block.\n",
    "\n",
    "    Collapses runs of whitespace and strips leading and trailing space.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHOOL_LINK_RE = re.compile(\n",
    "    r\"^/(?:en|ga)/cbes/(?P<SchoolID>\\d+)(?:/)?(?:\\?.*)?$\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "VOLUME_RE = re.compile(\n",
    "    r\"CB(?:E|É)S[\\s\\-]*([0-9A-Z]{3,5})\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "PCT_RE = re.compile(\n",
    "    r\"(\\d{1,3})\\s*%\"\n",
    ")\n",
    "\n",
    "NAME_RE = re.compile(\n",
    "    r\"(?:School|Scoil)\\s*:\\s*(.*?)\\s*CB(?:E|É)S\\b\",\n",
    "    re.IGNORECASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Parsing a single schools index page\n",
    "\n",
    "Each index page is parsed into a list of dictionaries, one per school card. The code below uses a small set of regular expressions to recognize:\n",
    "\n",
    "- the school ID from the link URL,\n",
    "- the CBES volume identifier from the card text, and\n",
    "- the percent transcribed from the card text.\n",
    "\n",
    "The HTML structure on Dúchas may change in the future, but the logic here is intended to be explicit and easy to adjust when that happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_school_cards(list_url):\n",
    "    \"\"\"\n",
    "    Parse a Dúchas schools index page into a list of row dictionaries,\n",
    "    with simple retry + backoff to handle 429 (Too Many Requests) responses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_url : str\n",
    "        URL for a schools index page.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        List of dictionaries with keys such as 'SchoolID', 'SchoolURL',\n",
    "        'SchoolName', 'VolumeRaw', and 'PctTranscribedRaw'. If all attempts\n",
    "        fail, returns an empty list and logs a warning.\n",
    "    \"\"\"\n",
    "    r = SCHOOLS_SESSION.get(list_url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    doc = html.fromstring(r.content)\n",
    "\n",
    "    rows = []\n",
    "    seen = set()\n",
    "\n",
    "    # KEY: use <li> cards, not generic //a\n",
    "    lis = doc.xpath(\"//li[.//a[contains(@href,'/cbes/')]]\")\n",
    "\n",
    "    for li in lis:\n",
    "        anchor = None\n",
    "        sid = None\n",
    "        url = None\n",
    "\n",
    "        # Find the first <a> that matches SCHOOL_LINK_RE\n",
    "        for a in li.xpath(\".//a[@href]\"):\n",
    "            href = a.get(\"href\") or \"\"\n",
    "            m = SCHOOL_LINK_RE.match(href)\n",
    "            if not m:\n",
    "                continue\n",
    "            sid = int(m.group(\"SchoolID\"))\n",
    "            url = href if href.startswith(\"http\") else f\"{BASE_URL}{href}\"\n",
    "            anchor = a\n",
    "            break\n",
    "\n",
    "        if sid is None or anchor is None or sid in seen:\n",
    "            continue\n",
    "        seen.add(sid)\n",
    "\n",
    "        # Full visible text of LI card\n",
    "        card_text = _clean_spaces(\" \".join(li.xpath(\".//text()\")))\n",
    "\n",
    "        # Name extraction (your old rules)\n",
    "        name = None\n",
    "        mname = NAME_RE.search(card_text)\n",
    "        if mname:\n",
    "            name = _clean_spaces(mname.group(1))\n",
    "        else:\n",
    "            atext = _clean_spaces(\" \".join(anchor.itertext()))\n",
    "            name = _clean_spaces(re.split(r\"\\s*CB(?:E|É)S\\b\", atext)[0] or atext)\n",
    "\n",
    "        # Volume number\n",
    "        vol_num = None\n",
    "        mvol = VOLUME_RE.search(card_text)\n",
    "        if mvol:\n",
    "            vol_num = mvol.group(1)\n",
    "\n",
    "        # Percent transcribed\n",
    "        percent = None\n",
    "        mp = PCT_RE.search(card_text)\n",
    "        if mp:\n",
    "            try:\n",
    "                v = int(mp.group(1))\n",
    "                if 0 <= v <= 100:\n",
    "                    percent = v\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        rows.append({\n",
    "            \"SchoolID\": sid,\n",
    "            \"SchoolURL\": url,\n",
    "            \"SchoolName\": name or None,\n",
    "            \"VolumeNumber\": vol_num,\n",
    "            \"PercentTranscribed\": percent,\n",
    "            \"ListPage\": list_url,\n",
    "        })\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Quick sanity check on a single page\n",
    "\n",
    "Before crawling all pages, it is helpful to inspect the parsed output for a single index page to confirm that the regular expressions and HTML selectors behave as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SchoolID</th>\n",
       "      <th>SchoolURL</th>\n",
       "      <th>SchoolName</th>\n",
       "      <th>VolumeNumber</th>\n",
       "      <th>PercentTranscribed</th>\n",
       "      <th>ListPage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4606380</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606380?Route=sc...</td>\n",
       "      <td>Cill Éinne</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4606492</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606492?Route=sc...</td>\n",
       "      <td>Fearainn an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4613911</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4613911?Route=sc...</td>\n",
       "      <td>Fearann an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4602668</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602668?Route=sc...</td>\n",
       "      <td>Inis Oirthir (Inisheer)</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4602669</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602669?Route=sc...</td>\n",
       "      <td>Breac-chluain</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SchoolID                                          SchoolURL  \\\n",
       "0   4606380  https://www.duchas.ie/en/cbes/4606380?Route=sc...   \n",
       "1   4606492  https://www.duchas.ie/en/cbes/4606492?Route=sc...   \n",
       "2   4613911  https://www.duchas.ie/en/cbes/4613911?Route=sc...   \n",
       "3   4602668  https://www.duchas.ie/en/cbes/4602668?Route=sc...   \n",
       "4   4602669  https://www.duchas.ie/en/cbes/4602669?Route=sc...   \n",
       "\n",
       "                SchoolName VolumeNumber  PercentTranscribed  \\\n",
       "0               Cill Éinne         0001                 100   \n",
       "1      Fearainn an Choirce         0001                 100   \n",
       "2       Fearann an Choirce         0001                 100   \n",
       "3  Inis Oirthir (Inisheer)         0001                 100   \n",
       "4            Breac-chluain         0001                 100   \n",
       "\n",
       "                                            ListPage  \n",
       "0  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "1  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "2  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "3  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "4  https://www.duchas.ie/en/cbes/schools?Page=1&P...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_page = next(iter(SCHOOL_PAGE_RANGE))\n",
    "test_url = build_school_index_url(test_page)\n",
    "\n",
    "test_rows = extract_school_cards(test_url)\n",
    "pd.DataFrame(test_rows).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Crawl all schools index pages\n",
    "\n",
    "The function below loops over all index pages, calls `extract_school_cards` on each, and concatenates the results into a single dataframe `df_schools`. A small sleep is inserted between requests to avoid overloading the server. The runtime and number of collected rows are reported at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_all_schools(pages, throttle_sec=0.15):\n",
    "    \"\"\"\n",
    "    Crawl all schools index pages and return a combined dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pages : iterable of int\n",
    "        Page numbers to crawl.\n",
    "    throttle_sec : float, optional\n",
    "        Sleep time between successive HTTP requests, by default 0.15 seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined, de-duplicated dataframe of schools.\n",
    "    \"\"\"\n",
    "    all_rows: List[Dict] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, page in enumerate(pages, start=1):\n",
    "        url = build_school_index_url(page)\n",
    "        rows = extract_school_cards(url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[{i} pages] last page={page}, total rows so far={len(all_rows)}, elapsed={elapsed:0.1f}s\")\n",
    "\n",
    "        time.sleep(throttle_sec)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # De-duplicate by SchoolID, keeping the first occurrence.\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"SchoolID\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 pages] last page=10, total rows so far=200, elapsed=13.6s\n",
      "[20 pages] last page=20, total rows so far=400, elapsed=28.0s\n",
      "[30 pages] last page=30, total rows so far=600, elapsed=42.4s\n",
      "[40 pages] last page=40, total rows so far=800, elapsed=56.9s\n",
      "[50 pages] last page=50, total rows so far=1000, elapsed=71.4s\n",
      "[60 pages] last page=60, total rows so far=1200, elapsed=86.2s\n",
      "[70 pages] last page=70, total rows so far=1400, elapsed=101.4s\n",
      "[80 pages] last page=80, total rows so far=1600, elapsed=116.1s\n",
      "[90 pages] last page=90, total rows so far=1800, elapsed=130.9s\n",
      "[100 pages] last page=100, total rows so far=2000, elapsed=145.5s\n",
      "[110 pages] last page=110, total rows so far=2200, elapsed=160.1s\n",
      "[120 pages] last page=120, total rows so far=2400, elapsed=174.8s\n",
      "[130 pages] last page=130, total rows so far=2600, elapsed=189.6s\n",
      "[140 pages] last page=140, total rows so far=2800, elapsed=204.3s\n",
      "[150 pages] last page=150, total rows so far=3000, elapsed=218.8s\n",
      "[160 pages] last page=160, total rows so far=3200, elapsed=235.1s\n",
      "[170 pages] last page=170, total rows so far=3400, elapsed=249.8s\n",
      "[180 pages] last page=180, total rows so far=3600, elapsed=264.3s\n",
      "[190 pages] last page=190, total rows so far=3800, elapsed=278.9s\n",
      "[200 pages] last page=200, total rows so far=4000, elapsed=293.6s\n",
      "[210 pages] last page=210, total rows so far=4200, elapsed=309.0s\n",
      "[220 pages] last page=220, total rows so far=4400, elapsed=323.4s\n",
      "Collected 4484 unique schools.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SchoolID</th>\n",
       "      <th>SchoolURL</th>\n",
       "      <th>SchoolName</th>\n",
       "      <th>VolumeNumber</th>\n",
       "      <th>PercentTranscribed</th>\n",
       "      <th>ListPage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4606380</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606380?Route=sc...</td>\n",
       "      <td>Cill Éinne</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4606492</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606492?Route=sc...</td>\n",
       "      <td>Fearainn an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4613911</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4613911?Route=sc...</td>\n",
       "      <td>Fearann an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4602668</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602668?Route=sc...</td>\n",
       "      <td>Inis Oirthir (Inisheer)</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4602669</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602669?Route=sc...</td>\n",
       "      <td>Breac-chluain</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SchoolID                                          SchoolURL  \\\n",
       "0   4606380  https://www.duchas.ie/en/cbes/4606380?Route=sc...   \n",
       "1   4606492  https://www.duchas.ie/en/cbes/4606492?Route=sc...   \n",
       "2   4613911  https://www.duchas.ie/en/cbes/4613911?Route=sc...   \n",
       "3   4602668  https://www.duchas.ie/en/cbes/4602668?Route=sc...   \n",
       "4   4602669  https://www.duchas.ie/en/cbes/4602669?Route=sc...   \n",
       "\n",
       "                SchoolName VolumeNumber  PercentTranscribed  \\\n",
       "0               Cill Éinne         0001                 100   \n",
       "1      Fearainn an Choirce         0001                 100   \n",
       "2       Fearann an Choirce         0001                 100   \n",
       "3  Inis Oirthir (Inisheer)         0001                 100   \n",
       "4            Breac-chluain         0001                 100   \n",
       "\n",
       "                                            ListPage  \n",
       "0  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "1  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "2  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "3  https://www.duchas.ie/en/cbes/schools?Page=1&P...  \n",
       "4  https://www.duchas.ie/en/cbes/schools?Page=1&P...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As observed on 2025-11-28: pages 1–225 return schools; 226+ are 404s.\n",
    "SCHOOL_PAGE_RANGE = range(1, 226)\n",
    "\n",
    "df_schools = crawl_all_schools(SCHOOL_PAGE_RANGE, throttle_sec=1.3)\n",
    "\n",
    "print(f\"Collected {len(df_schools)} unique schools.\")\n",
    "print(df_schools.shape[0])\n",
    "df_schools.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4484\n"
     ]
    }
   ],
   "source": [
    "print(df_schools.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "### 1.4 Post-processing and derived columns\n",
    "\n",
    "For the enrichment stage we need two additional columns:\n",
    "\n",
    "- `SchoolXML`: the XML endpoint for each school on Dúchas.\n",
    "- `SchoolURLClean`: the school URL without transient query parameters or routes.\n",
    "\n",
    "These derived columns are added here, and the resulting dataframe is saved to disk as the canonical school index table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4484, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SchoolID</th>\n",
       "      <th>SchoolURL</th>\n",
       "      <th>SchoolName</th>\n",
       "      <th>VolumeNumber</th>\n",
       "      <th>PercentTranscribed</th>\n",
       "      <th>ListPage</th>\n",
       "      <th>SchoolXML</th>\n",
       "      <th>SchoolURLClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4606380</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606380?Route=sc...</td>\n",
       "      <td>Cill Éinne</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "      <td>https://www.duchas.ie/xml/cbes/4606380</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4606492</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606492?Route=sc...</td>\n",
       "      <td>Fearainn an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "      <td>https://www.duchas.ie/xml/cbes/4606492</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4606492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4613911</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4613911?Route=sc...</td>\n",
       "      <td>Fearann an Choirce</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "      <td>https://www.duchas.ie/xml/cbes/4613911</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4613911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4602668</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602668?Route=sc...</td>\n",
       "      <td>Inis Oirthir (Inisheer)</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "      <td>https://www.duchas.ie/xml/cbes/4602668</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4602669</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602669?Route=sc...</td>\n",
       "      <td>Breac-chluain</td>\n",
       "      <td>0001</td>\n",
       "      <td>100</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/schools?Page=1&amp;P...</td>\n",
       "      <td>https://www.duchas.ie/xml/cbes/4602669</td>\n",
       "      <td>https://www.duchas.ie/en/cbes/4602669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SchoolID                                          SchoolURL  \\\n",
       "0   4606380  https://www.duchas.ie/en/cbes/4606380?Route=sc...   \n",
       "1   4606492  https://www.duchas.ie/en/cbes/4606492?Route=sc...   \n",
       "2   4613911  https://www.duchas.ie/en/cbes/4613911?Route=sc...   \n",
       "3   4602668  https://www.duchas.ie/en/cbes/4602668?Route=sc...   \n",
       "4   4602669  https://www.duchas.ie/en/cbes/4602669?Route=sc...   \n",
       "\n",
       "                SchoolName VolumeNumber  PercentTranscribed  \\\n",
       "0               Cill Éinne         0001                 100   \n",
       "1      Fearainn an Choirce         0001                 100   \n",
       "2       Fearann an Choirce         0001                 100   \n",
       "3  Inis Oirthir (Inisheer)         0001                 100   \n",
       "4            Breac-chluain         0001                 100   \n",
       "\n",
       "                                            ListPage  \\\n",
       "0  https://www.duchas.ie/en/cbes/schools?Page=1&P...   \n",
       "1  https://www.duchas.ie/en/cbes/schools?Page=1&P...   \n",
       "2  https://www.duchas.ie/en/cbes/schools?Page=1&P...   \n",
       "3  https://www.duchas.ie/en/cbes/schools?Page=1&P...   \n",
       "4  https://www.duchas.ie/en/cbes/schools?Page=1&P...   \n",
       "\n",
       "                                SchoolXML  \\\n",
       "0  https://www.duchas.ie/xml/cbes/4606380   \n",
       "1  https://www.duchas.ie/xml/cbes/4606492   \n",
       "2  https://www.duchas.ie/xml/cbes/4613911   \n",
       "3  https://www.duchas.ie/xml/cbes/4602668   \n",
       "4  https://www.duchas.ie/xml/cbes/4602669   \n",
       "\n",
       "                          SchoolURLClean  \n",
       "0  https://www.duchas.ie/en/cbes/4606380  \n",
       "1  https://www.duchas.ie/en/cbes/4606492  \n",
       "2  https://www.duchas.ie/en/cbes/4613911  \n",
       "3  https://www.duchas.ie/en/cbes/4602668  \n",
       "4  https://www.duchas.ie/en/cbes/4602669  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_school_derived_columns(df):\n",
    "    \"\"\"\n",
    "    Add derived columns needed for later enrichment:\n",
    "\n",
    "    - SchoolXML: XML endpoint for the school.\n",
    "    - SchoolURLClean: cleaned version of the HTML URL without query parameters.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # XML endpoints follow a simple pattern; adjust if needed.\n",
    "    df[\"SchoolXML\"] = \"https://www.duchas.ie/xml/cbes/\" + df[\"SchoolID\"].astype(str)\n",
    "\n",
    "    # Some SchoolURL values may have query parameters such as '?page=' or '?Route=schools'.\n",
    "    # Here we drop everything after the first '?'.\n",
    "    df[\"SchoolURLClean\"] = df[\"SchoolURL\"].str.split(\"?\", n=1).str[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_schools = add_school_derived_columns(df_schools)\n",
    "print(df_schools.shape)\n",
    "df_schools.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote schools index to ../data/duchas_schools_index.csv\n"
     ]
    }
   ],
   "source": [
    "# Adjust the path to fit your project structure.\n",
    "output_path_schools = \"../data/duchas_schools_index.csv\"\n",
    "df_schools.to_csv(output_path_schools, index=False)\n",
    "print(f\"Wrote schools index to {output_path_schools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the canonical school list\n",
    "\n",
    "The Schools Collection index was scraped using the original HTML/regex logic (which returns the full known set of 4,484 schools). The result is stored in:\n",
    "\n",
    "    ../data/duchas_schools_index.csv\n",
    "\n",
    "This file contains the complete SchoolID universe and is used as the starting point for all subsequent stages (XML enrichment, Logainm lookups, item index, and item-level scraping). It ensures full coverage even if the public Dúchas index pages expose only a partial subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "## 2. Enrich schools with XML and Logainm metadata\n",
    "\n",
    "The schools index provides only basic metadata about each school. Dúchas also exposes an XML endpoint for each school which contains additional information, including teacher names, roll numbers, and references to Logainm identifiers for places associated with the school. https://www.logainm.ie/en/\n",
    "\n",
    "In this section we construct an enrichment pipeline that:\n",
    "\n",
    "1. Retrieves the XML document for each school with a retry-aware HTTP client.\n",
    "2. Extracts a small set of fields from the XML, such as teacher name and roll number.\n",
    "3. Resolves any Logainm URL associated with the school and, when available, downloads the corresponding Well-Known Text (WKT) geometry for the place.\n",
    "\n",
    "The result is a new dataframe `df_schools_enriched` that extends `df_schools` with additional columns. The code is written to be robust against transient HTTP errors and to be easy to adapt if the XML schema evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import concurrent.futures as cf\n",
    "from functools import lru_cache\n",
    "\n",
    "from lxml import etree, html\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from typing import Dict, Any, Optional, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Robust HTTP session and XML helpers\n",
    "\n",
    "To avoid re-creating HTTP sessions for each request and to make the enrichment robust to transient errors, the code below uses a single `requests.Session` object with a retry policy and some small helper functions for fetching and parsing XML documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browser-like User-Agent used across all sessions\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "_thread_local = threading.local()\n",
    "\n",
    "def _build_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    Build a requests.Session with browser-like headers and retry logic,\n",
    "    to be used in a thread-local way.\n",
    "    \"\"\"\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "    try:\n",
    "        retry = Retry(\n",
    "            total=5,\n",
    "            connect=5,\n",
    "            read=5,\n",
    "            backoff_factor=0.4,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            raise_on_status=False,\n",
    "        )\n",
    "    except TypeError:\n",
    "        # For older urllib3 versions\n",
    "        retry = Retry(\n",
    "            total=5,\n",
    "            connect=5,\n",
    "            read=5,\n",
    "            backoff_factor=0.4,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            method_whitelist=frozenset([\"GET\"]),\n",
    "            raise_on_status=False,\n",
    "        )\n",
    "\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "def _session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    Return a thread-local Session, creating one if necessary.\n",
    "    \"\"\"\n",
    "    if not hasattr(_thread_local, \"session\"):\n",
    "        _thread_local.session = _build_session()\n",
    "    return _thread_local.session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_bytes(url: str, timeout: int = 30) -> bytes:\n",
    "    \"\"\"\n",
    "    Fetch raw bytes from a URL using the thread-local session.\n",
    "    \"\"\"\n",
    "    r = _session().get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "\n",
    "def _string_xp(root: etree._Element, xp: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Evaluate an XPath expression against an XML root and return a stripped string,\n",
    "    or None if not found / empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = root.xpath(xp)\n",
    "        if isinstance(s, str):\n",
    "            out = s.strip()\n",
    "        elif not s:\n",
    "            out = \"\"\n",
    "        else:\n",
    "            # Take the first value\n",
    "            val = s[0]\n",
    "            out = val.strip() if isinstance(val, str) else str(val).strip()\n",
    "        return out or None\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Logainm WKT helper\n",
    "\n",
    "Some school XML documents contain references to Logainm place records. The helper below downloads the corresponding HTML page from Logainm and tries to extract a WKT geometry from `data-wkt` attributes that are typically attached to Leaflet map elements. If no such attribute is present, the function returns `None`. This logic is deliberately conservative and can be refined later if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=4096)\n",
    "def get_logainm_wkt(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a Logainm (or related) page URL, attempt to extract a WKT geometry\n",
    "    from a data-wkt attribute.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        doc = html.fromstring(_fetch_bytes(url))\n",
    "        # Primary: Leaflet container with data-wkt\n",
    "        wkt = doc.xpath(\n",
    "            \"string(//*[contains(concat(' ', normalize-space(@class), ' '), \"\n",
    "            \"' leaflet-container ')][@data-wkt][1]/@data-wkt)\"\n",
    "        )\n",
    "        if not wkt:\n",
    "            # Fallback: first element with data-wkt\n",
    "            wkt = doc.xpath(\"string(//*[@data-wkt][1]/@data-wkt)\")\n",
    "        wkt = wkt.strip() if isinstance(wkt, str) else \"\"\n",
    "        return wkt or None\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Enriching a single school\n",
    "\n",
    "The function `enrich_school_row` takes a single row from `df_schools`, retrieves the associated XML document, and extracts a minimal set of fields. The exact XPath expressions depend on the Dúchas XML schema and can be refined as needed. At present we target:\n",
    "\n",
    "- teacher names (if available),\n",
    "- the default roll number, and\n",
    "- an associated Logainm URL plus its WKT geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_school_xml_to_row(school_xml_url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch a single SchoolXML and return a flat dict of fields.\n",
    "\n",
    "    Expected columns include volume metadata, page metadata, school location\n",
    "    (including Logainm WKT), teacher names, roll number, and some extra\n",
    "    top-level meta fields adapted to the actual schema (schoolName, county).\n",
    "    \"\"\"\n",
    "    row: Dict[str, Any] = {\n",
    "        \"SchoolXML\": school_xml_url,\n",
    "        # Volume metadata\n",
    "        \"volume_default\": None,\n",
    "        \"volume_listingOrder\": None,\n",
    "        \"volume_volumeNumber\": None,\n",
    "        \"volume_volumeStatus\": None,\n",
    "        # Page metadata\n",
    "        \"page_default\": None,\n",
    "        \"page_defaultURL\": None,\n",
    "        # SchoolLocation metadata\n",
    "        \"schoolLocation_default\": None,\n",
    "        \"schoolLocation_nameGA\": None,\n",
    "        \"schoolLocation_nameEN\": None,\n",
    "        \"schoolLocation_lat\": None,\n",
    "        \"schoolLocation_lon\": None,\n",
    "        \"schoolLocation_county\": None,\n",
    "        \"schoolLocation_url\": None,\n",
    "        \"schoolLocation_polygon\": None,\n",
    "        # Teacher info\n",
    "        \"teacherName_pretext\": None,\n",
    "        \"teacherName_text\": None,\n",
    "        \"teacherName_nameKey\": None,\n",
    "        # Roll number\n",
    "        \"schoolRollNumber_default\": None,\n",
    "        # Extra meta from the XML body\n",
    "        \"SchoolNameXML\": None,\n",
    "        \"RollNumber_XML\": None,\n",
    "        \"VolumeNumberXML\": None,\n",
    "        \"County_XML\": None,\n",
    "        \"Parish_XML\": None,\n",
    "        \"Barony_XML\": None,\n",
    "        \"Townland_XML\": None,\n",
    "        # Error, if any\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        root = etree.fromstring(_fetch_bytes(school_xml_url))\n",
    "\n",
    "        # Volume info\n",
    "        row[\"volume_default\"]      = _string_xp(root, \"string(//*[local-name()='volume']/@default)\")\n",
    "        row[\"volume_listingOrder\"] = _string_xp(root, \"string(//*[local-name()='volume']/@listingOrder)\")\n",
    "        row[\"volume_volumeNumber\"] = _string_xp(root, \"string(//*[local-name()='volume']/@volumeNumber)\")\n",
    "        row[\"volume_volumeStatus\"] = _string_xp(root, \"string(//*[local-name()='volume']/@volumeStatus)\")\n",
    "\n",
    "        # Page info\n",
    "        row[\"page_default\"]   = _string_xp(root, \"string(//*[local-name()='page']/@default)\")\n",
    "        row[\"page_defaultURL\"] = _string_xp(root, \"string(//*[local-name()='page']/@url)\")\n",
    "\n",
    "        # School location\n",
    "        row[\"schoolLocation_default\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@default)\"\n",
    "        )\n",
    "        row[\"schoolLocation_nameGA\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@nameGA)\"\n",
    "        )\n",
    "        row[\"schoolLocation_nameEN\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@nameEN)\"\n",
    "        )\n",
    "        row[\"schoolLocation_lat\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@lat)\"\n",
    "        )\n",
    "        row[\"schoolLocation_lon\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@lon)\"\n",
    "        )\n",
    "        row[\"schoolLocation_county\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolLocation']/@county)\"\n",
    "        )\n",
    "\n",
    "        loc_url = _string_xp(root, \"string(//*[local-name()='schoolLocation']/@url)\")\n",
    "        if loc_url:\n",
    "            loc_page = loc_url.replace(\"/xml/\", \"/en/\")\n",
    "            row[\"schoolLocation_url\"] = loc_page\n",
    "            row[\"schoolLocation_polygon\"] = get_logainm_wkt(loc_page)\n",
    "\n",
    "        # Teacher names\n",
    "        row[\"teacherName_pretext\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='teacherName']/@pretext)\"\n",
    "        )\n",
    "        row[\"teacherName_text\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='teacherName']/@text)\"\n",
    "        )\n",
    "        row[\"teacherName_nameKey\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='teacherName']/@nameKey)\"\n",
    "        )\n",
    "\n",
    "        # Roll number\n",
    "        row[\"schoolRollNumber_default\"] = _string_xp(\n",
    "            root, \"string(//*[local-name()='schoolRollNumber']/@default)\"\n",
    "        )\n",
    "\n",
    "        # Extra high-level meta adapted to the actual schema:\n",
    "        # <schoolName default=\"Cill Éinne\"/>\n",
    "        row[\"SchoolNameXML\"] = (\n",
    "            _string_xp(root, \"string(//*[local-name()='schoolName']/@default)\")\n",
    "            or _string_xp(root, \"string(//*[local-name()='schoolName'])\")\n",
    "        )\n",
    "\n",
    "        # RollNumber_XML and VolumeNumberXML are duplicates of existing fields\n",
    "        row[\"RollNumber_XML\"] = row[\"schoolRollNumber_default\"]\n",
    "        row[\"VolumeNumberXML\"] = row[\"volume_volumeNumber\"]\n",
    "\n",
    "        # County is stored as an attribute on schoolLocation (e.g., county=\"GA\")\n",
    "        row[\"County_XML\"] = row[\"schoolLocation_county\"]\n",
    "\n",
    "        # Parish / Barony / Townland do not appear in this schema snippet;\n",
    "        # leave them as None for now.\n",
    "        row[\"Parish_XML\"] = None\n",
    "        row[\"Barony_XML\"] = None\n",
    "        row[\"Townland_XML\"] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        row[\"error\"] = str(e)\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Enrich all schools\n",
    "\n",
    "The function `enrich_schools` applies `enrich_school_row` to every row in `df_schools` and returns an enriched dataframe keyed by `SchoolID`. For simplicity and transparency, the implementation below uses a straightforward loop; it can be parallelized later with `concurrent.futures` if needed.\n",
    "\n",
    "A small throttle is added between requests to avoid placing excessive load on the Dúchas and Logainm servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_schools(\n",
    "    df_schools: pd.DataFrame,\n",
    "    max_workers: int = 1,\n",
    "    throttle_sec: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich df_schools using SchoolXML URLs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_schools : pd.DataFrame\n",
    "        Must contain 'SchoolXML'.\n",
    "    max_workers : int\n",
    "        Maximum number of worker threads. For politeness, keep this small\n",
    "        (1–2) for this site.\n",
    "    throttle_sec : float\n",
    "        Sleep time after each XML fetch, in seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        df_schools merged with enrichment columns (one row per SchoolXML).\n",
    "    \"\"\"\n",
    "    urls = list(df_schools[\"SchoolXML\"].dropna().unique())\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    def work(u: str) -> Dict[str, Any]:\n",
    "        r = _parse_school_xml_to_row(u)\n",
    "        if throttle_sec:\n",
    "            time.sleep(throttle_sec)\n",
    "        return r\n",
    "\n",
    "    start_time = time.time()\n",
    "    total = len(urls)\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for i, row in enumerate(ex.map(work, urls), 1):\n",
    "            out_rows.append(row)\n",
    "            if i % 25 == 0 or i == total:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"...processed {i}/{total} schools (elapsed {elapsed:.1f}s)\")\n",
    "\n",
    "    df_enrich = pd.DataFrame(out_rows)\n",
    "    # Merge back on SchoolXML (one-to-one)\n",
    "    df_full = df_schools.merge(df_enrich, on=\"SchoolXML\", how=\"left\")\n",
    "    return df_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_in_batches(\n",
    "    df_schools: pd.DataFrame,\n",
    "    batch_size: int = 250,\n",
    "    max_workers: int = 1,\n",
    "    throttle_sec: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich df_schools in batches to be polite to the server and to allow\n",
    "    partial progress if the process is interrupted.\n",
    "    \"\"\"\n",
    "    enriched_chunks = []\n",
    "    n = len(df_schools)\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        print(f\"Processing schools {start}–{end-1} of {n}\")\n",
    "\n",
    "        chunk = df_schools.iloc[start:end].copy()\n",
    "        enriched_chunk = enrich_schools(\n",
    "            chunk,\n",
    "            max_workers=max_workers,\n",
    "            throttle_sec=throttle_sec,\n",
    "        )\n",
    "        enriched_chunks.append(enriched_chunk)\n",
    "\n",
    "        # Optional short cool-down between batches\n",
    "        time.sleep(120)\n",
    "\n",
    "    return pd.concat(enriched_chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_schools loaded from CSV and has SchoolXML column\n",
    "df_schools = pd.read_csv(\"../data/duchas_schools_index.csv\")\n",
    "df_schools[\"SchoolID\"] = df_schools[\"SchoolID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schools_full = enrich_in_batches(\n",
    "    df_schools,\n",
    "    batch_size=250,\n",
    "    max_workers=1,    # keep this at 1 for now\n",
    "    throttle_sec=2.0, # can relax to 0.8 later if things look stable\n",
    ")\n",
    "\n",
    "print(\"Enriched schools:\", len(df_schools_full))\n",
    "df_schools_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the enrichment has completed, the enriched data can be merged back into the original `df_schools` and written to disk as a single canonical table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote enriched schools table to ../data/duchas_schools_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "df_schools_full.to_csv(\"../data/duchas_schools_enriched.csv\", index=False)\n",
    "print(\"Wrote enriched schools table to ../data/duchas_schools_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "## 3. Per-school item titles\n",
    "\n",
    "In this section, the goal is to move from a school-level dataset to an item-level index of stories, essays, and other materials for each school.\n",
    "\n",
    "For each school, Dúchas exposes pages that list individual items, with links to the item view. Here we treat the school’s main CBES page as the entry point for collecting item links. We parse these pages, identify links to individual items, and construct a dataframe `df_items_index` with one row per item, containing (at minimum) school ID, page ID, item ID, item URL, and item title.\n",
    "\n",
    "The exact HTML structure on Dúchas can change, so the XPath and URL patterns in this section should be considered a documented starting point that can be updated if the site evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Item URLs on Dúchas CBES typically look something like:\n",
    "#   /en/cbes/<SchoolID>/<PageID>/<ItemID>\n",
    "# or a closely related pattern. The regex below captures those IDs.\n",
    "ITEM_LINK_RE = re.compile(\n",
    "    r\"/en/cbes/(?P<SchoolID>\\d+)/(?P<PageID>\\d+)(?:/(?P<ItemID>\\d+))?\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_school_items_url(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Construct the URL that lists items for a given school.\n",
    "\n",
    "    At present we use the cleaned school URL itself as the entry point,\n",
    "    assuming that the main CBES page contains links to all items associated\n",
    "    with the school (possibly with pagination).\n",
    "\n",
    "    If Dúchas provides a dedicated 'Titles/Teidil' view or uses specific\n",
    "    query parameters, adjust this function to return the appropriate URL.\n",
    "    \"\"\"\n",
    "    return row[\"SchoolURLClean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items_for_school(row: pd.Series) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract item links for a single school from its Dúchas CBES page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        School row with at least 'SchoolID', 'SchoolName', and 'SchoolURLClean'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        List of item dictionaries with fields such as:\n",
    "        SchoolID, PageID, ItemID, ItemURL, ItemTitle, SchoolName.\n",
    "    \"\"\"\n",
    "    school_id = str(row[\"SchoolID\"])\n",
    "    school_name = row.get(\"SchoolName\")\n",
    "    url = build_school_items_url(row)\n",
    "\n",
    "    try:\n",
    "        resp = ENRICH_SESSION.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print(f\"[WARN] Failed to fetch items page for school {school_id} at {url}: {exc}\")\n",
    "        return []\n",
    "\n",
    "    tree = html.fromstring(resp.content)\n",
    "\n",
    "    # Strategy: look for anchors that match the item URL pattern and treat\n",
    "    # the anchor text as the item title. You may want to tighten this by\n",
    "    # restricting to specific containers if the page has many links.\n",
    "    anchors = tree.xpath(\"//a[contains(@href, '/en/cbes/')]\")\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    for a in anchors:\n",
    "        href = a.get(\"href\", \"\")\n",
    "        m = ITEM_LINK_RE.search(href)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        item_school_id = m.group(\"SchoolID\")\n",
    "        page_id = m.group(\"PageID\")\n",
    "        item_id = m.group(\"ItemID\")\n",
    "\n",
    "        # Only keep items that belong to this school.\n",
    "        if item_school_id != school_id:\n",
    "            continue\n",
    "\n",
    "        item_url = BASE_URL + href\n",
    "        title_text = _clean_spaces(\" \".join(a.itertext()))\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"SchoolID\": item_school_id,\n",
    "                \"PageID\": page_id,\n",
    "                \"ItemID\": item_id,\n",
    "                \"ItemURL\": item_url,\n",
    "                \"ItemTitle\": title_text or None,\n",
    "                \"SchoolName\": school_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Collecting items for all schools\n",
    "\n",
    "The function below applies `extract_items_for_school` to every school in `df_schools_full`. For transparency and ease of debugging the implementation uses a simple thread pool over schools. This balances speed and readability and makes it straightforward to throttle the rate of requests.\n",
    "\n",
    "If Dúchas paginates item lists across multiple pages for a school, the current implementation will only capture item links on the main CBES page. In that case the `extract_items_for_school` function can be extended to follow additional pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_items(df_schools_base, max_workers=8, throttle_sec=0.05):\n",
    "    \"\"\"\n",
    "    Collect item links for all schools in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_schools_base : pd.DataFrame\n",
    "        Dataframe of schools. Must contain 'SchoolID', 'SchoolName',\n",
    "        and 'SchoolURLClean'.\n",
    "    max_workers : int\n",
    "        Maximum number of worker threads used to fetch school pages.\n",
    "    throttle_sec : float\n",
    "        Delay between submissions of tasks, as a light throttle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with one row per item (SchoolID, PageID, ItemID, ItemURL,\n",
    "        ItemTitle, SchoolName), de-duplicated by (SchoolID, PageID, ItemID).\n",
    "    \"\"\"\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    def _worker(row_tuple):\n",
    "        idx, row = row_tuple\n",
    "        return extract_items_for_school(row)\n",
    "\n",
    "    school_rows = list(df_schools_base.reset_index(drop=True).iterrows())\n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "        futures = {}\n",
    "        for i, row_tuple in enumerate(school_rows, start=1):\n",
    "            fut = pool.submit(_worker, row_tuple)\n",
    "            futures[fut] = row_tuple[0]  # index of the school row\n",
    "\n",
    "            time.sleep(throttle_sec)\n",
    "\n",
    "        for j, fut in enumerate(concurrent.futures.as_completed(futures), start=1):\n",
    "            idx = futures[fut]\n",
    "            try:\n",
    "                rows = fut.result()\n",
    "                records.extend(rows)\n",
    "            except Exception as exc:\n",
    "                print(f\"[WARN] items worker failed for school index {idx}: {exc}\")\n",
    "\n",
    "            if j % 50 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[{j} schools processed] total items so far={len(records)}, elapsed={elapsed:0.1f}s\")\n",
    "\n",
    "    df_items = pd.DataFrame(records)\n",
    "    if not df_items.empty:\n",
    "        df_items = df_items.drop_duplicates(subset=[\"SchoolID\", \"PageID\", \"ItemID\"]).reset_index(drop=True)\n",
    "    return df_items\n",
    "\n",
    "\n",
    "df_items_index = collect_all_items(df_schools_full, max_workers=8, throttle_sec=0.05)\n",
    "print(f\"Collected {len(df_items_index)} unique items.\")\n",
    "df_items_index.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save the item index\n",
    "output_path_items_index = \"../data/duchas_items_index.csv\"\n",
    "df_items_index.to_csv(output_path_items_index, index=False)\n",
    "print(f\"Wrote items index to {output_path_items_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "## 4. Per-item HTML and XML scraper\n",
    "\n",
    "With an item index in hand, the next step is to enrich each item with more detailed metadata. Dúchas typically provides an HTML view of the item and a corresponding XML representation that includes structural and linguistic information.\n",
    "\n",
    "In this section we construct a per-item parser that:\n",
    "\n",
    "1. Fetches the HTML view of an item and extracts its main text and any relevant metadata that are convenient to read from HTML.\n",
    "2. Builds the corresponding XML URL for the item, fetches it, and extracts machine-readable fields that are only present in XML (for example counts of tokens, languages, or structural annotation).\n",
    "3. Returns a dictionary for each item that can be assembled into a dataframe `df_items_full`.\n",
    "\n",
    "The exact XPaths depend on the Dúchas HTML and XML schema; the focus here is to provide a clear, easily modifiable skeleton rather than hard-coding every detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def build_item_xml_url(item_url):\n",
    "    \"\"\"\n",
    "    Construct an XML URL for an item given its HTML URL.\n",
    "\n",
    "    For CBES items, the XML URL is often parallel to the HTML URL, with the\n",
    "    path switched from '/en/cbes/...' to '/xml/cbes/...'. This function\n",
    "    applies that transformation.\n",
    "\n",
    "    Adjust if Dúchas uses a different convention.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(item_url)\n",
    "    # Replace '/en/cbes/' with '/xml/cbes/' in the path.\n",
    "    xml_path = parsed.path.replace(\"/en/cbes/\", \"/xml/cbes/\")\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}{xml_path}\"\n",
    "\n",
    "\n",
    "def parse_item_html(item_url):\n",
    "    \"\"\"\n",
    "    Fetch and parse the HTML representation of an item.\n",
    "\n",
    "    Returns a dictionary with HTML-derived fields such as the visible title\n",
    "    and main text. XPaths are intentionally conservative and may be adjusted\n",
    "    to match the actual site structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = ENRICH_SESSION.get(item_url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print(f\"[WARN] Failed to fetch item HTML at {item_url}: {exc}\")\n",
    "        return {\n",
    "            \"ItemHTMLTitle\": None,\n",
    "            \"ItemHTMLText\": None,\n",
    "        }\n",
    "\n",
    "    tree = html.fromstring(resp.content)\n",
    "\n",
    "    # Title: use the main heading on the item page, if present.\n",
    "    title_nodes = tree.xpath(\"//h1 | //h2\")\n",
    "    html_title = None\n",
    "    if title_nodes:\n",
    "        html_title = _clean_spaces(\" \".join(title_nodes[0].itertext()))\n",
    "\n",
    "    # Main text: many Dúchas pages put item text in a main content div.\n",
    "    # This is a placeholder XPath; adjust to match the actual class / id.\n",
    "    text_nodes = tree.xpath(\"//div[contains(@class, 'transcription') or contains(@class, 'content')]\")\n",
    "    if text_nodes:\n",
    "        html_text = _clean_spaces(\" \".join(text_nodes[0].itertext()))\n",
    "    else:\n",
    "        html_text = None\n",
    "\n",
    "    return {\n",
    "        \"ItemHTMLTitle\": html_title,\n",
    "        \"ItemHTMLText\": html_text,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item_xml(item_xml_url):\n",
    "    \"\"\"\n",
    "    Fetch and parse the XML representation of an item.\n",
    "\n",
    "    Returns a dictionary with XML-derived fields. The concrete XPaths used\n",
    "    here are placeholders and should be adapted once the XML schema is\n",
    "    inspected in more detail.\n",
    "    \"\"\"\n",
    "    xml_root = fetch_xml(item_xml_url)\n",
    "    if xml_root is None:\n",
    "        return {\n",
    "            \"ItemXMLAvailable\": False,\n",
    "            \"ItemTokenCount\": None,\n",
    "            \"ItemLanguage\": None,\n",
    "        }\n",
    "\n",
    "    # Example: count word-like elements (e.g. <w> nodes).\n",
    "    tokens = xml_root.xpath(\"//w\")\n",
    "    token_count = len(tokens) if tokens is not None else None\n",
    "\n",
    "    # Example: guess language from an attribute or element.\n",
    "    # Replace this with the actual path once known.\n",
    "    lang = string_xp(xml_root, \"//language/text()\") or string_xp(xml_root, \"//@xml:lang\")\n",
    "\n",
    "    return {\n",
    "        \"ItemXMLAvailable\": True,\n",
    "        \"ItemTokenCount\": token_count,\n",
    "        \"ItemLanguage\": lang,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item_record(row: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Enrich a single item record from df_items_index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row with at least 'SchoolID', 'PageID', 'ItemID', 'ItemURL', and 'ItemTitle'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the original IDs and URL, plus HTML- and XML-\n",
    "        derived fields.\n",
    "    \"\"\"\n",
    "    item_url = row[\"ItemURL\"]\n",
    "    item_xml_url = build_item_xml_url(item_url)\n",
    "\n",
    "    base = {\n",
    "        \"SchoolID\": row[\"SchoolID\"],\n",
    "        \"PageID\": row[\"PageID\"],\n",
    "        \"ItemID\": row[\"ItemID\"],\n",
    "        \"ItemURL\": item_url,\n",
    "        \"ItemXMLURL\": item_xml_url,\n",
    "        \"ItemTitleIndex\": row.get(\"ItemTitle\"),\n",
    "        \"SchoolName\": row.get(\"SchoolName\"),\n",
    "    }\n",
    "\n",
    "    html_fields = parse_item_html(item_url)\n",
    "    xml_fields = parse_item_xml(item_xml_url)\n",
    "\n",
    "    out = base.copy()\n",
    "    out.update(html_fields)\n",
    "    out.update(xml_fields)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Enrich all items\n",
    "\n",
    "The function below applies `parse_item_record` to every row in `df_items_index` and returns an enriched dataframe `df_items_full`. For clarity it uses a simple thread pool and a small throttle; this can be adjusted depending on how aggressively you want to crawl.\n",
    "\n",
    "Because per-item scraping is more intensive than school-level or index-level requests, it is especially important to monitor runtime and request volume, which is the focus of Section 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_all_items(df_items_base, max_workers=8, throttle_sec=0.02):\n",
    "    \"\"\"\n",
    "    Enrich all items in df_items_index by applying parse_item_record.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_items_base : pd.DataFrame\n",
    "        Base items dataframe with one row per item.\n",
    "    max_workers : int\n",
    "        Maximum number of worker threads.\n",
    "    throttle_sec : float\n",
    "        Delay between task submissions as a light throttle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Enriched items dataframe.\n",
    "    \"\"\"\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    def _worker(row_tuple):\n",
    "        idx, row = row_tuple\n",
    "        return parse_item_record(row)\n",
    "\n",
    "    item_rows = list(df_items_base.reset_index(drop=True).iterrows())\n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "        futures = {}\n",
    "        for i, row_tuple in enumerate(item_rows, start=1):\n",
    "            fut = pool.submit(_worker, row_tuple)\n",
    "            futures[fut] = row_tuple[0]\n",
    "\n",
    "            time.sleep(throttle_sec)\n",
    "\n",
    "        for j, fut in enumerate(concurrent.futures.as_completed(futures), start=1):\n",
    "            idx = futures[fut]\n",
    "            try:\n",
    "                rec = fut.result()\n",
    "                records.append(rec)\n",
    "            except Exception as exc:\n",
    "                print(f\"[WARN] item worker failed for item index {idx}: {exc}\")\n",
    "\n",
    "            if j % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[{j} items processed] elapsed={elapsed:0.1f}s\")\n",
    "\n",
    "    df_items_full = pd.DataFrame(records)\n",
    "    # De-duplicate just in case.\n",
    "    if not df_items_full.empty:\n",
    "        df_items_full = df_items_full.drop_duplicates(\n",
    "            subset=[\"SchoolID\", \"PageID\", \"ItemID\"]\n",
    "        ).reset_index(drop=True)\n",
    "    return df_items_full\n",
    "\n",
    "\n",
    "# Example: start with a small subset until you are happy with the parsing.\n",
    "df_items_full_sample = enrich_all_items(df_items_index.head(50), max_workers=8, throttle_sec=0.02)\n",
    "df_items_full_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Once satisfied, run on the full items index.\n",
    "# # This can take time depending on the size of df_items_index.\n",
    "\n",
    "# df_items_full = enrich_all_items(df_items_index, max_workers=8, throttle_sec=0.02)\n",
    "# output_path_items_full = \"../data/duchas_items_full.csv\"\n",
    "# df_items_full.to_csv(output_path_items_full, index=False)\n",
    "# print(f\"Wrote full items table to {output_path_items_full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "## 5. Request budgeting and benchmarking\n",
    "\n",
    "The full pipeline involves a substantial number of HTTP requests: index pages for schools, per-school item lists, and per-item HTML and XML documents.\n",
    "\n",
    "Before running the entire per-item enrichment, it is helpful to estimate the total number of requests and to benchmark the runtime on a small sample of items. This section provides simple helpers for both tasks.\n",
    "\n",
    "The goal is to make the cost of a full run explicit, so it is easier to decide on throttling parameters and whether to split the crawl into smaller batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_item_request_counts(df_items_base):\n",
    "    \"\"\"\n",
    "    Estimate the number of HTTP requests needed for per-item enrichment.\n",
    "\n",
    "    For each item, the current pipeline makes:\n",
    "      - one HTML request (item page),\n",
    "      - one XML request (item XML).\n",
    "\n",
    "    Additional requests (for example, page-level XML shared across items)\n",
    "    can be added later if needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_items_base : pd.DataFrame\n",
    "        Items index dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with counts of estimated requests.\n",
    "    \"\"\"\n",
    "    n_items = len(df_items_base)\n",
    "\n",
    "    # One HTML and one XML request per item in the current design.\n",
    "    n_html = n_items\n",
    "    n_xml = n_items\n",
    "\n",
    "    total = n_html + n_xml\n",
    "\n",
    "    return {\n",
    "        \"n_items\": n_items,\n",
    "        \"n_item_html_requests\": n_html,\n",
    "        \"n_item_xml_requests\": n_xml,\n",
    "        \"n_total_item_requests\": total,\n",
    "    }\n",
    "\n",
    "\n",
    "est_counts = estimate_item_request_counts(df_items_index)\n",
    "est_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_item_enrichment(df_items_base, sample_size=50, max_workers=8, throttle_sec=0.02):\n",
    "    \"\"\"\n",
    "    Benchmark the per-item enrichment on a small random sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_items_base : pd.DataFrame\n",
    "        Items index dataframe.\n",
    "    sample_size : int\n",
    "        Number of items to sample for the benchmark.\n",
    "    max_workers : int\n",
    "        Maximum number of worker threads.\n",
    "    throttle_sec : float\n",
    "        Delay between task submissions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Benchmark statistics including time per item and projected total time.\n",
    "    \"\"\"\n",
    "    if len(df_items_base) == 0:\n",
    "        raise ValueError(\"df_items_base is empty; nothing to benchmark.\")\n",
    "\n",
    "    sample = df_items_base.sample(\n",
    "        n=min(sample_size, len(df_items_base)),\n",
    "        random_state=42\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    _ = enrich_all_items(sample, max_workers=max_workers, throttle_sec=throttle_sec)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    n_sample = len(sample)\n",
    "    time_per_item = elapsed / n_sample\n",
    "    projected_total = time_per_item * len(df_items_base)\n",
    "\n",
    "    return {\n",
    "        \"sample_size\": n_sample,\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "        \"time_per_item_seconds\": time_per_item,\n",
    "        \"projected_total_seconds\": projected_total,\n",
    "        \"projected_total_hours\": projected_total / 3600.0,\n",
    "    }\n",
    "\n",
    "\n",
    "benchmark_stats = benchmark_item_enrichment(\n",
    "    df_items_index,\n",
    "    sample_size=50,\n",
    "    max_workers=8,\n",
    "    throttle_sec=0.02,\n",
    ")\n",
    "benchmark_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
